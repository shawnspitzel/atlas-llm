_wandb:
    value:
        cli_version: 0.21.2
        e:
            odqgkmv3koq1qqwmsg0e03lchzfprava:
                args:
                    - --batch_size=512
                    - --beta1=0.9
                    - --beta2=0.999
                    - --checkpoint_dir=atlas-llm/src/checkpoints/model/gpt2
                    - --checkpoint_interval=500
                    - --compile=True
                    - --context_length=1024
                    - --cosine_iters=10000
                    - --d_ff=3072
                    - --d_model=768
                    - --data_dtype=uint16
                    - --device=cuda
                    - --eval_interval=100
                    - --eval_iters=20
                    - --gradient_clip_norm=1
                    - --learning_rate=0.00025
                    - --log_interval=10
                    - --max_iters=10000
                    - --min_lr=2.5e-05
                    - --num_heads=12
                    - --num_layers=12
                    - --optimizer=adamw
                    - --resume_from=None
                    - --seed=42
                    - --theta=10000
                    - --train_data=atlas-llm/src/data/tokenized/owt/train/owt_train_tokens_.bin
                    - --val_data=atlas-llm/src/data/tokenized/owt/val/owt_valid_val_tokens_.bin
                    - --vocab_size=50257
                    - --warmup_iters=375
                    - --weight_decay=0.01
                codePath: atlas-llm/src/training/pretrain.py
                codePathLocal: src/training/pretrain.py
                cpu_count: 8
                cpu_count_logical: 16
                cudaVersion: "12.6"
                disk:
                    /:
                        total: "1081101176832"
                        used: "46971174912"
                email: shawnspitzel@gmail.com
                executable: /mnt/c/Users/kevin/CS336/atlas-llm/atlas-llm/.venv/bin/python
                git:
                    commit: 938b99d74c46a6195a375395f6a0978efef3229c
                    remote: https://github.com/shawnspitzel/atlas-llm.git
                gpu: NVIDIA GeForce RTX 4060 Ti
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Ada
                      memoryTotal: "8585740288"
                      name: NVIDIA GeForce RTX 4060 Ti
                      uuid: GPU-79c3433b-e693-a218-47a0-5c430515be6c
                host: DESKTOP-DBTREBA
                memory:
                    total: "16729194496"
                os: Linux-6.6.87.1-microsoft-standard-WSL2-x86_64-with-glibc2.39
                program: /mnt/c/Users/kevin/CS336/atlas-llm/atlas-llm/src/training/pretrain.py
                python: CPython 3.12.3
                root: /mnt/c/Users/kevin/CS336/atlas-llm/atlas-llm
                startedAt: "2026-01-19T07:20:18.044076Z"
                writerId: odqgkmv3koq1qqwmsg0e03lchzfprava
        m:
            - "1": iteration
              "6":
                - 3
              "7": []
            - "2": train/*
              "5": 1
              "6":
                - 1
              "7": []
            - "2": val/*
              "5": 1
              "6":
                - 1
              "7": []
        python_version: 3.12.3
        t:
            "1":
                - 1
            "2":
                - 1
            "3":
                - 2
                - 7
                - 13
                - 14
                - 16
            "4": 3.12.3
            "5": 0.21.2
            "12": 0.21.2
            "13": linux-x86_64
batch_size:
    value: 512
beta1:
    value: 0.9
beta2:
    value: 0.999
checkpoint_dir:
    value: atlas-llm/src/checkpoints/model/gpt2
checkpoint_interval:
    value: 500
compile:
    value: true
context_length:
    value: 1024
cosine_iters:
    value: 10000
d_ff:
    value: 3072
d_model:
    value: 768
data_dtype:
    value: uint16
device:
    value: cuda
eval_interval:
    value: 100
eval_iters:
    value: 20
gradient_clip_norm:
    value: 1
learning_rate:
    value: 0.00025
log_interval:
    value: 10
max_iters:
    value: 10000
min_lr:
    value: 2.5e-05
num_heads:
    value: 12
num_layers:
    value: 12
optimizer:
    value: adamw
profile:
    value: false
profile_output:
    value: checkpoints/profile_stats.prof
resume_from:
    value: null
seed:
    value: 42
theta:
    value: 10000
train_data:
    value: atlas-llm/src/data/tokenized/owt/train/owt_train_tokens_.bin
use_params:
    value: false
use_wandb:
    value: false
val_data:
    value: atlas-llm/src/data/tokenized/owt/val/owt_valid_val_tokens_.bin
vocab_size:
    value: 50257
wandb_project:
    value: cs336-assignment1
wandb_run_name:
    value: null
warmup_iters:
    value: 375
weight_decay:
    value: 0.01
