================================================================================
LINE-BY-LINE PROFILE (line_profiler)
================================================================================

Timer unit: 1e-09 s

Total time: 354.389 s
File: /mnt/c/Users/kevin/CS336/atlas-llm/atlas-llm/src/training/pretrain.py
Function: pretrain at line 33

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    33                                           def pretrain(model, train_data: np.array, val_data: np.array, optimizer, params, iteration: int):
    34      1500 2281666015.0 1.52e+06      0.6      model.train()
    35      1500     3.17e+10 2.11e+07      8.9      inputs, targets = data_loading(train_data, params["batch_size"], params["context_length"], params["device"])
    36                                           
    37      1500  700002400.0 466668.3      0.2      optimizer.zero_grad()
    38      1500     1.04e+11 6.94e+07     29.4      logits = model(inputs)
    39      1500  334534345.0 223022.9      0.1      loss = cross_entropy_loss(logits, targets)
    40      1500     3.33e+10 2.22e+07      9.4      loss.backward()
    41                                           
    42      1500     8.07e+10 5.38e+07     22.8      gradient_clipping(model.parameters(), max_norm=params['gradient_clip_norm'])
    43                                           
    44      3000   14885820.0   4961.9      0.0      lr = learning_rate_schedule(
    45      1500    1989963.0   1326.6      0.0          curr_iter=iteration,
    46      1500    2135254.0   1423.5      0.0          max_lr=params["learning_rate"],
    47      1500    1399077.0    932.7      0.0          min_lr=params["min_lr"],
    48      1500    1658942.0   1106.0      0.0          warm_iters=params["warmup_iters"],
    49      1500     856400.0    570.9      0.0          cos_iters=params["cosine_iters"]
    50                                                   )
    51      3000    4408547.0   1469.5      0.0      for param_group in optimizer.param_groups:
    52      1500    1636548.0   1091.0      0.0          param_group['lr'] = lr
    53      1500 7318761045.0 4.88e+06      2.1      optimizer.step()
    54                                           
    55      1500      2.3e+10 1.53e+07      6.5      train_loss = loss.item()
    56      1500    4074322.0   2716.2      0.0      val_loss = None
    57                                           
    58      1500    4251034.0   2834.0      0.0      if iteration % params["eval_interval"] == 0:
    59       150  250552266.0 1.67e+06      0.1          model.eval()
    60       150     281349.0   1875.7      0.0          val_losses = []
    61                                           
    62       300   12139515.0  40465.1      0.0          with torch.no_grad():
    63      1650    4484993.0   2718.2      0.0              for _ in range(params["eval_iters"]):
    64      3000     1.44e+10  4.8e+06      4.1                  val_inputs, val_targets = data_loading(
    65      1500    1712983.0   1142.0      0.0                      val_data,
    66      1500    1779248.0   1186.2      0.0                      params["batch_size"],
    67      1500    1811197.0   1207.5      0.0                      params["context_length"],
    68      1500    1291059.0    860.7      0.0                      params["device"]
    69                                                           )
    70      1500     3.06e+10 2.04e+07      8.6                  val_logits = model(val_inputs)
    71      1500  165886895.0 110591.3      0.0                  val_loss_batch = cross_entropy_loss(val_logits, val_targets)
    72      1500     2.53e+10 1.69e+07      7.1                  val_losses.append(val_loss_batch.item())
    73       150    1376510.0   9176.7      0.0          val_loss = sum(val_losses) / len(val_losses)
    74       150  237786489.0 1.59e+06      0.1          model.train()
    75                                           
    76      1500    6420128.0   4280.1      0.0      return train_loss, val_loss

Total time: 586.741 s
File: /mnt/c/Users/kevin/CS336/atlas-llm/atlas-llm/src/training/pretrain.py
Function: run at line 79

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    79                                           def run(params):
    80         1   89197880.0 8.92e+07      0.0      torch.manual_seed(params["seed"])
    81         1  195263860.0 1.95e+08      0.0      np.random.seed(params["seed"])
    82         1  100892448.0 1.01e+08      0.0      if torch.cuda.is_available():
    83         1    7795073.0  7.8e+06      0.0          torch.cuda.manual_seed_all(params["seed"])
    84                                           
    85         1    5358113.0 5.36e+06      0.0      checkpoint_dir = get_checkpoint_dir(params)
    86         1    1273747.0 1.27e+06      0.0      print(f"Checkpoints will be saved to: {checkpoint_dir}")
    87                                           
    88         1        898.0    898.0      0.0      requested_device = params["device"]
    89         1      35961.0  35961.0      0.0      if requested_device == "cuda" and not torch.cuda.is_available():
    90                                                   print("Warning: CUDA requested but not available. Falling back to CPU.")
    91                                                   device = torch.device("cpu")
    92         1        623.0    623.0      0.0      elif requested_device == "mps" and not torch.backends.mps.is_available():
    93                                                   print("Warning: MPS requested but not available. Falling back to CPU.")
    94                                                   device = torch.device("cpu")
    95                                               else:
    96         1      14456.0  14456.0      0.0          device = torch.device(requested_device)
    97                                           
    98         3       6756.0   2252.0      0.0      for name, path in [("train_data", params["train_data"]), ("val_data", params["val_data"])]:
    99         2    7209894.0  3.6e+06      0.0          if not os.path.exists(path):
   100                                                       raise FileNotFoundError(f"{name} file not found: {path}")
   101         2    6670531.0 3.34e+06      0.0          if os.path.getsize(path) == 0:
   102                                                       raise ValueError(f"{name} file is empty: {path}")
   103                                           
   104         1  136326146.0 1.36e+08      0.0      train_data = np.memmap(params["train_data"], dtype=params["data_dtype"], mode='r')
   105         1  136654330.0 1.37e+08      0.0      val_data = np.memmap(params["val_data"], dtype=params["data_dtype"], mode='r')
   106                                               
   107         2  683053117.0 3.42e+08      0.1      model = Transformer(    
   108         1        899.0    899.0      0.0              d_model=params['d_model'],
   109         1        587.0    587.0      0.0              num_heads=params['num_heads'],
   110         1        798.0    798.0      0.0              d_ff=params['d_ff'],
   111         1        541.0    541.0      0.0              num_layers=params['num_layers'],
   112         1        449.0    449.0      0.0              vocab_size=params['vocab_size'],
   113         1        403.0    403.0      0.0              context_length=params['context_length'],
   114         1        459.0    459.0      0.0              theta=params['theta']
   115                                                   )
   116                                           
   117         1  475562314.0 4.76e+08      0.1      model = model.to(device)
   118         1       1833.0   1833.0      0.0      if params["compile"]:
   119         1     3.95e+10 3.95e+10      6.7          model = torch.compile(model)
   120                                                   
   121         1        999.0    999.0      0.0      if params["optimizer"] == "sgd":
   122                                                   optimizer = SGDOptimizer(params=model.parameters(), lr=params["learning_rate"])
   123                                               else:
   124         2    1567605.0 783802.5      0.0          optimizer = AdamW(
   125         1       1521.0   1521.0      0.0              params=model.parameters(), 
   126         1        788.0    788.0      0.0              betas=(params["beta1"], params["beta2"]),
   127         1        495.0    495.0      0.0              weight_decay=params["weight_decay"],
   128         1        486.0    486.0      0.0              lr=params["learning_rate"]
   129                                                       )
   130                                           
   131         1        504.0    504.0      0.0      start_iter = 0
   132         1        486.0    486.0      0.0      if params["resume_from"]:
   133                                                   resume_path = params["resume_from"]
   134                                                   start_iter = load_checkpoint(resume_path, model, optimizer=optimizer, device=device)
   135                                           
   136         2    5356302.0 2.68e+06      0.0      with tqdm(total=params["max_iters"], desc="Training", unit=" iters", initial=start_iter) as pbar:
   137      1501    1154876.0    769.4      0.0          for iter in range(start_iter, params["max_iters"]):
   138      3000     3.55e+11 1.18e+08     60.4              train_loss, val_loss = pretrain(
   139      1500     790446.0    527.0      0.0                  model=model,
   140      1500     712770.0    475.2      0.0                  train_data=train_data,
   141      1500     672173.0    448.1      0.0                  val_data=val_data,
   142      1500     705928.0    470.6      0.0                  optimizer=optimizer,
   143      1500     662927.0    442.0      0.0                  params=params,
   144      1500     562157.0    374.8      0.0                  iteration=iter
   145                                                           )
   146      1500    4416004.0   2944.0      0.0              if wandb.run is not None:
   147      1500    1057106.0    704.7      0.0                  try:
   148      1500    4462974.0   2975.3      0.0                      log_dict = {
   149      1500    1378051.0    918.7      0.0                          "train/loss": train_loss,
   150      1500    2766145.0   1844.1      0.0                          "lr": optimizer.param_groups[0]["lr"],
   151      1500    1008301.0    672.2      0.0                          "iteration": iter
   152                                                               }
   153      1500    1291897.0    861.3      0.0                      if val_loss is not None:
   154       150      77247.0    515.0      0.0                          log_dict["val/loss"] = val_loss
   155      1500  958339962.0 638893.3      0.2                      wandb.log(log_dict)
   156                                                           except Exception as e:
   157                                                               print(f"Warning: Failed to log metrics to W&B: {e}")
   158                                           
   159      1500    1349286.0    899.5      0.0              if val_loss is not None:
   160       300  230878874.0 769596.2      0.0                  pbar.set_postfix({
   161       150     651846.0   4345.6      0.0                      'train_loss': f'{train_loss:.4f}',
   162       150     280467.0   1869.8      0.0                      'val_loss': f'{val_loss:.4f}',
   163       150     731169.0   4874.5      0.0                      'lr': f'{optimizer.param_groups[0]["lr"]:.2e}'
   164                                                           })
   165                                                       else:
   166      2700 2045309895.0 757522.2      0.3                  pbar.set_postfix({
   167      1350    5574794.0   4129.5      0.0                      'train_loss': f'{train_loss:.4f}',
   168      1350    7058591.0   5228.6      0.0                      'lr': f'{optimizer.param_groups[0]["lr"]:.2e}'
   169                                                           })
   170      1500 1608849212.0 1.07e+06      0.3              pbar.update(1)
   171                                           
   172      1500    3094689.0   2063.1      0.0              if iter > 0 and iter % params["checkpoint_interval"] == 0:
   173        29     833210.0  28731.4      0.0                  checkpoint_path = osp.join(checkpoint_dir, f"checkpoint_iter_{iter}.pt")
   174        58     1.85e+11 3.19e+09     31.5                  save_checkpoint(
   175        29      21023.0    724.9      0.0                      model=model,
   176        29      17962.0    619.4      0.0                      optimizer=optimizer,
   177        29      11464.0    395.3      0.0                      iteration=iter,
   178        29      10897.0    375.8      0.0                      out=checkpoint_path
   179                                                           )
   180        29  122098167.0 4.21e+06      0.0                  pbar.write(f"Saved checkpoint to {checkpoint_path}")
   181                                           
   182         1      25529.0  25529.0      0.0      output_path = osp.join(checkpoint_dir, f"checkpoint_iter_{params['max_iters']}.pt")
   183         1       2411.0   2411.0      0.0      if params["max_iters"] % params["checkpoint_interval"] != 0:
   184                                                   save_checkpoint(
   185                                                           model=model,
   186                                                           optimizer=optimizer,
   187                                                           iteration=params["max_iters"],
   188                                                           out=output_path
   189                                                       )
   190         1     896635.0 896635.0      0.0      print(f"Model saved to {output_path}")
   191         1  895818859.0 8.96e+08      0.2      wandb.finish()

