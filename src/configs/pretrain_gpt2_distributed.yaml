program: src/training/distributed_pretrain.py
project: AtlasLM-Pretrain-Distributed-GPT2
method: grid
metric:
  name: val/loss
  goal: minimize

command:
  - ${env}
  - ${interpreter}
  - ${program}
  - ${args}

parameters:
  d_model:
    value: 768
  num_heads:
    value: 12
  d_ff:
    value: 3072  # 4 * d_model
  num_layers:
    value: 12
  vocab_size:
    value: 50257
  context_length:
    value: 1024
  theta:
    value: 10000.0

  batch_size:
    value: 8  # Per-GPU batch size. Total batch = 8 * num_gpus. Increase if you have more GPU memory
  max_iters:
    value: 10000
  eval_interval:
    value: 100
  eval_iters:
    value: 20
  log_interval:
    value: 10
  checkpoint_interval:
    value: 500
  seed:
    value: 42

  optimizer:
    value: adamw
  learning_rate:
    value: 2.5e-4 
  min_lr:
    value: 2.5e-5 
  weight_decay:
    value: 0.01
  beta1:
    value: 0.9
  beta2:
    value: 0.999 
  gradient_clip_norm:
    value: 1.0

  warmup_iters:
    value: 375
  cosine_iters:
    value: 10000

  train_data:
    value: src/data/tokenized/owt/train/owt_train_tokens_.bin
  val_data:
    value: src/data/tokenized/owt/val/owt_valid_val_tokens_.bin
  data_dtype:
    value: uint16

  checkpoint_dir:
    value: src/checkpoints/model/gpt2

  resume_from:
    value: null

  device:
    value: cuda  # Will be overridden per rank in distributed mode
  compile:
    value: false  # Set to false if OOM or compilation errors occur
